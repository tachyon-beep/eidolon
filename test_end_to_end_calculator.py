"""
End-to-End Pipeline Test: Calculator Program

Tests the complete MONAD pipeline from user request to working code:
1. User Request ‚Üí SystemDecomposer
2. Subsystem Tasks ‚Üí SubsystemDecomposer
3. Module Tasks ‚Üí ModuleDecomposer
4. Function Tasks ‚Üí FunctionPlanner
5. Code Generation ‚Üí ImplementationOrchestrator
6. Validation ‚Üí Quality Assessment

Metrics tracked:
- Pipeline completion rate
- Code quality (docstrings, type hints, error handling)
- Correctness (runs without errors, produces expected output)
- Test coverage (has tests, tests pass)
- Best practices (PEP 8, security)
"""

import asyncio
import sys
import os
from pathlib import Path
import tempfile
import shutil
import subprocess
from typing import Dict, Any, List

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent / "backend"))

from llm_providers import OpenAICompatibleProvider
from planning.decomposition import SystemDecomposer
from agents.orchestrator import AgentOrchestrator
from logging_config import get_logger

logger = get_logger(__name__)


class QualityAssessment:
    """Assess quality of generated code"""

    @staticmethod
    def assess_python_file(file_path: Path) -> Dict[str, Any]:
        """
        Assess quality of a Python file

        Returns:
            Dict with quality metrics
        """
        if not file_path.exists():
            return {
                "exists": False,
                "error": "File not found"
            }

        content = file_path.read_text()
        lines = content.split('\n')

        # Count metrics
        metrics = {
            "exists": True,
            "lines": len(lines),
            "has_docstrings": '"""' in content or "'''" in content,
            "has_type_hints": '->' in content or ': str' in content or ': int' in content or ': float' in content,
            "has_error_handling": 'try:' in content or 'except' in content or 'raise' in content,
            "has_functions": 'def ' in content,
            "has_classes": 'class ' in content,
            "has_main": 'if __name__' in content,
            "function_count": content.count('def '),
            "class_count": content.count('class '),
            "import_count": content.count('import ') + content.count('from '),
            "comment_lines": sum(1 for line in lines if line.strip().startswith('#')),
            "blank_lines": sum(1 for line in lines if not line.strip()),
        }

        # Check for security issues
        security_issues = []
        if 'eval(' in content:
            security_issues.append("Uses eval() - security risk")
        if 'exec(' in content:
            security_issues.append("Uses exec() - security risk")
        if 'os.system(' in content:
            security_issues.append("Uses os.system() - security risk")

        metrics["security_issues"] = security_issues
        metrics["is_secure"] = len(security_issues) == 0

        # Calculate quality score (0-100)
        score = 0
        if metrics["has_docstrings"]:
            score += 20
        if metrics["has_type_hints"]:
            score += 20
        if metrics["has_error_handling"]:
            score += 20
        if metrics["has_main"]:
            score += 10
        if metrics["is_secure"]:
            score += 20
        if metrics["function_count"] >= 3:
            score += 10

        metrics["quality_score"] = score

        return metrics

    @staticmethod
    def test_python_syntax(file_path: Path) -> Dict[str, Any]:
        """Test if Python file has valid syntax"""
        try:
            result = subprocess.run(
                ["python", "-m", "py_compile", str(file_path)],
                capture_output=True,
                text=True,
                timeout=5
            )

            return {
                "valid_syntax": result.returncode == 0,
                "error": result.stderr if result.returncode != 0 else None
            }
        except Exception as e:
            return {
                "valid_syntax": False,
                "error": str(e)
            }

    @staticmethod
    def run_python_file(file_path: Path, stdin_input: str = None) -> Dict[str, Any]:
        """Run a Python file and capture output"""
        try:
            result = subprocess.run(
                ["python", str(file_path)],
                input=stdin_input,
                capture_output=True,
                text=True,
                timeout=10
            )

            return {
                "success": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": "Timeout - program took too long"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    @staticmethod
    def assess_calculator(calc_file: Path) -> Dict[str, Any]:
        """
        Specific assessment for calculator program

        Tests:
        1. Has add, subtract, multiply, divide functions
        2. Handles division by zero
        3. Runs without errors
        """
        content = calc_file.read_text()

        assessment = {
            "has_add": 'def add' in content,
            "has_subtract": 'def subtract' in content or 'def sub' in content,
            "has_multiply": 'def multiply' in content or 'def mul' in content,
            "has_divide": 'def divide' in content or 'def div' in content,
            "handles_div_zero": 'ZeroDivisionError' in content or 'if.*== 0' in content or 'if.*!= 0' in content,
        }

        # Calculate completeness score
        required_functions = ["has_add", "has_subtract", "has_multiply", "has_divide"]
        completeness = sum(assessment[key] for key in required_functions) / len(required_functions) * 100

        assessment["completeness"] = completeness
        assessment["all_operations"] = all(assessment[key] for key in required_functions)

        return assessment


async def run_end_to_end_test():
    """Run complete end-to-end test of calculator creation"""

    print("\n" + "="*80)
    print("END-TO-END PIPELINE TEST: CALCULATOR PROGRAM")
    print("="*80)

    # Load API key
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        print("‚ùå ERROR: OPENROUTER_API_KEY not found in environment")
        return False

    # Initialize LLM provider
    llm_provider = OpenAICompatibleProvider(
        api_key=api_key,
        base_url=os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1"),
        model=os.getenv("OPENROUTER_MODEL", "x-ai/grok-4.1-fast:free")
    )

    print(f"\n‚úì LLM Provider: {llm_provider.model}")
    print(f"‚úì Test: Create calculator program from scratch")

    # Create temporary project directory
    project_dir = Path(tempfile.mkdtemp(prefix="monad_calculator_"))
    print(f"‚úì Project Directory: {project_dir}")

    try:
        # User request
        user_request = """Create a command-line calculator program with the following features:
1. Add two numbers
2. Subtract two numbers
3. Multiply two numbers
4. Divide two numbers (with division by zero handling)
5. Interactive menu for user to choose operation
6. Input validation
7. Loop to perform multiple calculations
"""

        print(f"\nüìù User Request:\n{user_request}")

        # Initialize orchestrator
        orchestrator = TaskOrchestrator(llm_provider)

        print("\n" + "-"*80)
        print("PHASE 1: TASK DECOMPOSITION")
        print("-"*80)

        # Run the full orchestration
        result = await orchestrator.orchestrate(
            user_request=user_request,
            project_path=str(project_dir)
        )

        print(f"\n‚úì Orchestration completed")
        print(f"‚úì Status: {result.get('status', 'unknown')}")
        print(f"‚úì Tasks completed: {result.get('tasks_completed', 0)}")
        print(f"‚úì Tasks failed: {result.get('tasks_failed', 0)}")

        # Assess the generated code
        print("\n" + "-"*80)
        print("PHASE 2: CODE QUALITY ASSESSMENT")
        print("-"*80)

        # Find generated Python files
        python_files = list(project_dir.rglob("*.py"))
        print(f"\n‚úì Generated {len(python_files)} Python files:")
        for pf in python_files:
            print(f"  - {pf.relative_to(project_dir)}")

        if not python_files:
            print("\n‚ùå ERROR: No Python files generated!")
            return False

        # Assess main calculator file (assume it's the largest or named calculator.py)
        calc_file = None
        for pf in python_files:
            if 'calculator' in pf.name.lower() or 'calc' in pf.name.lower():
                calc_file = pf
                break

        if not calc_file and python_files:
            # Use the largest file
            calc_file = max(python_files, key=lambda p: p.stat().st_size)

        print(f"\nüìÑ Assessing main file: {calc_file.name}")

        # Quality assessment
        quality = QualityAssessment.assess_python_file(calc_file)

        print(f"\n**Code Quality Metrics:**")
        print(f"  Lines of code: {quality['lines']}")
        print(f"  Functions: {quality['function_count']}")
        print(f"  Classes: {quality['class_count']}")
        print(f"  Has docstrings: {'‚úÖ' if quality['has_docstrings'] else '‚ùå'}")
        print(f"  Has type hints: {'‚úÖ' if quality['has_type_hints'] else '‚ùå'}")
        print(f"  Has error handling: {'‚úÖ' if quality['has_error_handling'] else '‚ùå'}")
        print(f"  Has main block: {'‚úÖ' if quality['has_main'] else '‚ùå'}")
        print(f"  Security: {'‚úÖ Secure' if quality['is_secure'] else '‚ùå Issues found'}")
        if quality['security_issues']:
            for issue in quality['security_issues']:
                print(f"    - {issue}")
        print(f"\n  **Quality Score: {quality['quality_score']}/100**")

        # Calculator-specific assessment
        calc_assessment = QualityAssessment.assess_calculator(calc_file)

        print(f"\n**Calculator Completeness:**")
        print(f"  Add function: {'‚úÖ' if calc_assessment['has_add'] else '‚ùå'}")
        print(f"  Subtract function: {'‚úÖ' if calc_assessment['has_subtract'] else '‚ùå'}")
        print(f"  Multiply function: {'‚úÖ' if calc_assessment['has_multiply'] else '‚ùå'}")
        print(f"  Divide function: {'‚úÖ' if calc_assessment['has_divide'] else '‚ùå'}")
        print(f"  Division by zero handling: {'‚úÖ' if calc_assessment['handles_div_zero'] else '‚ùå'}")
        print(f"\n  **Completeness: {calc_assessment['completeness']:.0f}%**")

        # Syntax check
        print("\n" + "-"*80)
        print("PHASE 3: SYNTAX VALIDATION")
        print("-"*80)

        syntax_check = QualityAssessment.test_python_syntax(calc_file)
        print(f"\n  Valid Python syntax: {'‚úÖ' if syntax_check['valid_syntax'] else '‚ùå'}")
        if not syntax_check['valid_syntax']:
            print(f"  Error: {syntax_check['error']}")

        # Try to run it (if it has a main block)
        if quality['has_main'] and syntax_check['valid_syntax']:
            print("\n" + "-"*80)
            print("PHASE 4: EXECUTION TEST")
            print("-"*80)

            # We can't really run interactive program, but we can check if it starts
            print("\n  Note: Interactive program detected (requires user input)")
            print("  Checking if program starts without errors...")

            # Try importing it instead
            try:
                import importlib.util
                spec = importlib.util.spec_from_file_location("calculator", calc_file)
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    # Don't actually load it if it has __name__ == "__main__" that runs
                    print("  ‚úÖ Module can be imported")
            except Exception as e:
                print(f"  ‚ùå Import error: {e}")

        # Overall assessment
        print("\n" + "="*80)
        print("OVERALL ASSESSMENT")
        print("="*80)

        metrics = {
            "pipeline_completed": result.get('status') == 'completed',
            "files_generated": len(python_files) > 0,
            "valid_syntax": syntax_check['valid_syntax'],
            "has_all_operations": calc_assessment['all_operations'],
            "quality_score": quality['quality_score'],
            "completeness": calc_assessment['completeness'],
        }

        # Calculate overall success score
        success_score = 0
        if metrics["pipeline_completed"]:
            success_score += 30
        if metrics["files_generated"]:
            success_score += 10
        if metrics["valid_syntax"]:
            success_score += 20
        if metrics["has_all_operations"]:
            success_score += 20
        if metrics["quality_score"] >= 60:
            success_score += 20

        print(f"\n**Success Metrics:**")
        print(f"  Pipeline completed: {'‚úÖ' if metrics['pipeline_completed'] else '‚ùå'}")
        print(f"  Files generated: {'‚úÖ' if metrics['files_generated'] else '‚ùå'}")
        print(f"  Valid syntax: {'‚úÖ' if metrics['valid_syntax'] else '‚ùå'}")
        print(f"  All operations present: {'‚úÖ' if metrics['has_all_operations'] else '‚ùå'}")
        print(f"  Quality score ‚â•60: {'‚úÖ' if metrics['quality_score'] >= 60 else '‚ùå'}")

        print(f"\n  **OVERALL SUCCESS: {success_score}/100**")

        # Recommendations
        print("\n" + "-"*80)
        print("RECOMMENDATIONS FOR HARDENING")
        print("-"*80)

        issues = []
        if not metrics["pipeline_completed"]:
            issues.append("Pipeline failed to complete - check orchestrator error handling")
        if not metrics["valid_syntax"]:
            issues.append("Generated code has syntax errors - improve FunctionPlanner")
        if not metrics["has_all_operations"]:
            issues.append("Missing required operations - improve SystemDecomposer completeness")
        if metrics["quality_score"] < 60:
            issues.append("Low quality score - improve code generation prompts")
        if not quality['has_docstrings']:
            issues.append("Missing docstrings - add to FunctionPlanner prompts")
        if not quality['has_type_hints']:
            issues.append("Missing type hints - add to code generation requirements")
        if not quality['has_error_handling']:
            issues.append("Missing error handling - improve robustness prompts")

        if issues:
            print("\n‚ùå Issues to address:")
            for i, issue in enumerate(issues, 1):
                print(f"  {i}. {issue}")
        else:
            print("\n‚úÖ No major issues found! System performing well.")

        # Show what needs Phase 2.5 integration
        print("\n" + "-"*80)
        print("PHASE 2.5 INTEGRATION STATUS")
        print("-"*80)

        print("\n  ‚úÖ SystemDecomposer - INTEGRATED")
        print("  ‚è≥ SubsystemDecomposer - NEEDS INTEGRATION")
        print("  ‚è≥ ModuleDecomposer - NEEDS INTEGRATION")
        print("  ‚è≥ ClassDecomposer - NEEDS INTEGRATION")
        print("  ‚è≥ FunctionPlanner - NEEDS INTEGRATION")

        # Show generated code
        print("\n" + "-"*80)
        print("GENERATED CODE PREVIEW")
        print("-"*80)

        print(f"\nFile: {calc_file.name}")
        print("-" * 40)
        content = calc_file.read_text()
        all_lines = content.split('\n')
        lines = all_lines[:50]  # First 50 lines
        for i, line in enumerate(lines, 1):
            print(f"{i:3d} | {line}")
        if len(all_lines) > 50:
            remaining = len(all_lines) - 50
            print(f"... ({remaining} more lines)")

        return success_score >= 60

    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        # Cleanup (optional - leave for inspection)
        print(f"\nüíæ Project saved to: {project_dir}")
        print("   (Remove manually if no longer needed)")


if __name__ == "__main__":
    success = asyncio.run(run_end_to_end_test())

    print("\n" + "="*80)
    print("TEST COMPLETE")
    print("="*80)

    if success:
        print("\n‚úÖ END-TO-END TEST PASSED")
        print("\nThe calculator program was successfully generated!")
        sys.exit(0)
    else:
        print("\n‚ùå END-TO-END TEST FAILED")
        print("\nSee recommendations above for hardening priorities.")
        sys.exit(1)
